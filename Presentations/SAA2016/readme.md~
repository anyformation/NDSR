# Web Archiving Tools at Rhizome

One of the projects I worked on during my residency was assisting my NDSR host site mentor at Rhizome to preserve works in the "Archive as Artwork" collection. This collection includes artworks created on the Instagram and Tumblr social media platforms, as well as hand-coded websites.  We used a combination of tools to address the challenges  for capture posed by the combination of dynamic content and the large # of overall pages in many of the sites.  
[Archive as Artwork Collection]
(https://rhizome.org/art/artbase/collections/collection-artist-made-web-archives/)

An additional challenge posed by the Art of The Artwork collection is that some of these artworks are ongoing projects. "Veteranas and Rucas", for example, has added many posts since the archived state we created.  Rhizome hosted an event at the New Museum with a panel discussion about many of the issues involved in collecting social media work. The recording of that even can be viewed here on Livestream:

["Who Owns Digital Social Memory?"] (http://livestream.com/newmuseum/events/4837386)

### wget
Wget runs in the command line and provides many options for scoping crawls of websites. 

Wget is very well-documented online, and the manual is great place to start. The GNU Wget manual is here: 

[GNU Wget Manual] (http://www.delorie.com/gnu/docs/wget/wget.html#SEC_Top)

Within the manual, the [examples] (http://www.delorie.com/gnu/docs/wget/wget_29.html) page was especially helpful, and walks through examples at three levels of complexity. 

The Archive Team wiki's Wget page is another great reference to check for Wget. This page offers an introduction to using wget and helpful advice about some common problems. It also has a list of essays and other sources on wget. There are also instructions for installing wget in Windows 7, which I did not try but might be useful to have! 

[Archive Team wiki Wget] (http://www.archiveteam.org/index.php?title=Wget) 

### webrecorder

Webrecorder allowed us to explore the artworks visually, browsing the site as a user while also capturing pages as we went. Because Webrecorder runs in the browser, javascript on a page is executed as the users explores the site, and many resources that would not be captured with wget are retrieved. This process is very time-consuming, however. For many sites, the overall number of pages makes this method impractical, which is why combining wget with Webrecorder, via pywb as described below, worked well. 

Webrecorder is currently in development, and addressing what role automation could play in the process of "symmetrical web archiving" is one of the questions that the project is exploring. The Webrecorder team from Rhizome recently gave a presentation about how Webrecorder fits into Rhizome's preservation tools; these slides points to some of differences in this approach to web archiving. 

[Tools for Preserving Performative Media] (http://labs.rhizome.org/presentations/talk-20160722#/)  

One of the larger issues at stake here, though, is how to define the boundaries of anything within web archiving processes. This article in the NYT Magazine in June 2016 sketched out some particularly relevant considerations within the scale and inevitable incompleteness of web archiving, for who and what will have their history preserved. 

[How an Archive of the Internet Could Change History] (http://www.nytimes.com/2016/06/26/magazine/how-an-archive-of-the-internet-could-change-history.html?_r=0)

### pywb
Pywb is a suite of tools for building collections of captured WARC files, and replaying the captured web content inside the collections. 

I used the documentation provided at Ilya's github repository for pywb to learn how to use pywb. I especially recommend the tutorial he created for creating and managing collections, which is here: 

[Auto Configuration and Web Archives Collections Manager] (https://github.com/ikreymer/pywb/wiki/Auto-Configuration-and-Web-Archive-Collections-Manager)

I wrote a post on the NDSR-NY blog which is a simplified version of Ilya's documentation, and has screenshots for walking step-by-step through the instructions for installing python, piptools, and pywb, and for using some of pywb's functions. 

[Pywb post on NDSR-NY blog]
(http://ndsr.nycdigital.org/shared-conferences-diy-web-archives/)

### Other Resources

This paper presents one of the most thorough and clearly-defined explanations I found about the problems of client-side technologies for web archiving, and also proposes a crawl method for capturing resources that are missing from most crawls. Although the scope of this method exceeds the individually-scaled methods I've discussed here, the literature review and definitions of terms provided by this paper are an essential frame of reference for anyone interested in learning more about web archiving. 

[Adapting the Hypercube Model to Archive Deferred Representations and Their Descendants] (http://arxiv.org/abs/1601.05142)


